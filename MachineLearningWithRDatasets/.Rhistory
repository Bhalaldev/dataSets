help()
mean(abs(rnorm(100)))
mean(abs(rnorm(100)))
rnorm(100)
abs(rnorm(10)
quit
abs(rnorm(10))
plot(abs(rnorm(10)))
plot(rnorm(10))
x <- c(1,2,4)
q <- c(x,x,7)
q
x
x[3]
q[5]
x
x[2:3]
mean(x)
sd(x)
avg(x)
y<- mean(x)
y
clear(q)
y # print out y
setwd("~/R Files")
data()
mean(Nile)
hist(Nile)
hist(women)
plot(women)
hist(women,breaks=12)
plot(women,breaks=12)
q()
read.csv(kerrSdgePowerUseage.csv)
read.csv("KerrSdgePowerUseage.csv"")
read.csv("KerrSdgePowerUseage.csv")
str(KerrSdgePowerUseage.csv)
str("KerrSdgePowerUseage.csv"")
str("KerrSdgePowerUseage.csv")
pwr_tbl <- read.csv("KerrSdgePowerUseage.csv")
str(pwr_tbl)
pwr_tbl
kwh <- pwr_tbl[,4]
kwh
summary(kwh)
d+d
2+2
install.packages("robust")
data(breslow.dat, package="robust")
names(breslow.dat)
str(breslow.dat)
library(ggplot2)
load(diamonds)
data(diamonds)
plot(diamonds$carat)
hist(diamonds$carat, xlab = 'Carat')
plot(diamonds$carat, diamonds$price)
ggplot(diamonds=diamonds) + geom_histogram(aes(x=carat))
ggplot(data=diamonds) + geom_histogram(aes(x=carat))
ggplot(data=diamonds) + geom_density(aes(x=carat))
ggplot(data=diamonds) + geom_density(aes(x=carat), fill='grey50')
ggplot(data=diamonds) + geom_density(aes(x=carat), fill='grey40')
ggplot(data=diamonds) + geom_density(aes(x=carat), fill='grey20')
ggplot(data=diamonds) + geom_density(aes(x=carat), fill='grey60')
ggplot(diamonds, aes(x=carat,y=price)) + geom_point()
g <- ggplot(diamonds,aes(x=carat,y=price))
g + geom_point(aes(color=color))
g + geom_point()
g + geom_point(aes(color=color))
g + geom_point(aes(color=carat))
str(diamonds)
g + geom_point(aes(color=clarity))
say.hello <- function() {
print("Hello, World!")
}
say.hello
say.hello()
sprintf('Hello' %s, 'World')
sprintf('Hello %s', 'World')
hello.person <- function(name) {
sprintf('Hello %s', 'name')
}
hello.person(Macy)
hello.person <- function(name) {
sprintf('Hello %s', name)
}
hello.person(Macy)
hello.person("Macy")
double.num <- function(x)
{
x * 2
}
double.num(4)
double.num <- function(x)
{
return(x * 2)
}
double.num(4)
double.list <- function(list)
{
return(list * 2)
}
x <- c(1,2,3,4)
double.list(x)
```{r}
summary(cars)
plot(cars)
df <- data.frame(x = 1:3, y = 3:1, z = letters[1:3])
df
df[df$x == 2,]
melt(df)
library(reshape2)
melt(df)
df[c(1,3),]
df
df[df$x == 3,]
mtcars
str(mtcars)
mtcars[mtcars$cyl == 4,]
mtcars[mtcars$hp > 100,]
mtcars[mtcars$hp > 200,]
head(mtcars)
names(mtcars)
mtcars[is.na(mtcars$mpg)]
is.na(mtcars$mgs)
apply(mtcars,2,is.na)
apply(mtcars,2,is.na)
x <- c(1,1,2,3,5,8)
x[c(T,T,F,F,T,T)]
a <- matrix(1:9, nrow = 3)
a
colnames(a) <- c("A","B", "C")
a
a[c(T,F,T), c("B","A")]
df
str(df)
df[df$y > 1,]
mtcars
head(mtcars)
mtcars[mtcars$cyl == 8]
mtcars[,mtcars$cyl == 8]
mtcars[,mtcars$cyl == 8,]
mtcars[mtcars$cyl == 8,]
mtcars[, mtcars$cyl == 8]
mtcars[mtcars$cyl == 8,]
melt(mtcars)
help(pam)
help(cluster)
library(cluster)
help(cluster)
df1 = data.frame(CustomerId=c(1:6),Product=c(rep("Toaster",3),rep("Radio",3)))
df2 = data.frame(CustomerId=c(2,4,6),State=c(rep("Alabama",2),rep("Ohio",1)))
df1
df2
outer <- merge(x = df1, y = df2, by = "CustomerId", all = TRUE)
outer
outer[is.na(outer$State)]
outer$State
outer[is.na(,outer$State)]
outer[is.na(outer$State)]
is.na(outer$State)
outer$State(is.na(outer$State))
outer$State[is.na(outer$State)]
outer$State[is.na(outer$State)] <- 0
outer$State[is.na(outer$State) <- 0]
outer
left.outer <- merge(x = df1, y = df2, by = "CustomerId", all.x=TRUE)
left.outer
right.outer <- merge(x = df1, y = df2, by = "CustomerId", all.y=TRUE)
right.outer
cross.join <- merge(x = df1, y = df2, by = NULL)
cross.join
letters
letters[c(1,5,9)]
letters[30]
letters[-c(1,2,3)]
str(letters[-c(1,2,3)])
x <- c("a","b","c","d")
x
x[TRUE,FALSE,TRUE,FALSE]
x[c(TRUE,FALSE,TRUE,FALSE)]
help(c)
letters[2]
letters[2,3]
letters[c(2,3)]
x == 'b'
x[x=='b']
setwd("/Users/stuart/Desktop/BigDataDocumentation/DataSets/MachineLearningWithRDatasets/")
library(e1071)     # For SVN classification using Hastei Book
library(gmodels)   # For CrossTables
set.seed(1)
x <- matrix(rnorm(20*2),ncol=2)    # create data matrix
y <- c(rep(-1,10),rep(1,10))       # create label vector
y==1
x[y==1,]
x[y==1,]+1
x
y
y==1
x[y==1]
x[y==1,]
x <- matrix(rnorm(20*2),ncol=2)    # create data matrix
y <- c(rep(-1,10),rep(1,10))       # create label vector
x
x[y==1,] = x[y==1,] + 1            # What does this do???
x
plot(x,col=3-y)                    # From this plot, the answer is no
dat <- data.frame(x=x, y= as.factor(y))
library(e1071)
svmfit <- svm(y~., data=dat, kernel="linear", cost=10, scale=FALSE)
plot(svmfit,dat)
svmfit$index
CrossTable(x = y, y = svmfit$fitted, prop.chisq=FALSE)
x <- c("a","b","c","d")
x[x=='b']
x=='b'
1:length(x)
c(1,2,3,4)["TRUE","FALSE","TRUE","FALSE"]
c(1,2,3,4)[c("TRUE","FALSE","TRUE","FALSE")]
c(1,2,3,4) [c("TRUE","FALSE","TRUE","FALSE")]
x=='b'
x[x=='b']
help(rnorm)
x <- rnorm(100)
x
x[x<0.3 & x > -0.1]
x[ !i]
x[!x<0.3]
x[]
length(x)
x <- c(1,2,3)
x
x[c(1,3)] <- 10
x
x[]<- 1
x
x <- matrix[1:6,2,3]
x <- matrix(1:6,2,3)
x
apply(x,1,sum)
apply(x,2,sum)
apply(x,1,mean)
rep(x=1,10)
rep(2,10)
x <- rep(2,10)
x
height = c(70, 72, 67, 66, 75, 64, 66, 68, 63, 65)
gender <- rep(c("Male","Female"),(5,5))
gender <- rep(c("Male","Female"),c(5,5))
gender
by(FUN=mean,gender,height)
by(FUN=mean, IND=gender,data=height)
setwd("/Users/stuart/Desktop/BigDataDocumentation/DataSets/MachineLearningWithRDatasets/")
# Load Libraries
library(gmodels)   # For CrossTables
library(class)     # For kNN classification
library(kernlab)   # For SVN classification
library(e1071)     # For SVN classification using Hastei Book
usedcars <- read.csv("chapter 2/usedcars.csv", stringsAsFactors = FALSE)
usedcars$conservative <- usedcars$color %in% c("Black","Gray","Silver","White")
table(usedcars$conservative)
CrossTable(x = usedcars$model, y=usedcars$conservative)
set.seed(1)
x <- matrix(rnorm(20*2),ncol=2)    # create data matrix
y <- c(rep(-1,10),rep(1,10))       # create label vector
x[y==1,] = x[y==1,] + 1            # What does this do? - Adds 1 to those x.1 and x.2 in bottom half of x
plot(x,col=3-y)                    # From this plot, the answer is no
x
dat <- data.frame(x=x, y= as.factor(y))
library(e1071)
svmfit <- svm(y~., data=dat, kernel="linear", cost=10, scale=FALSE)
plot(svmfit,dat)
CrossTable(x = y, y = svmfit$fitted, prop.chisq=FALSE)
xtest <- matrix(rnorm(20*2),ncol=2)
ytest <- sample(c(-1,1),20,rep=TRUE)
ytest
xtest
xtest
xtest[ytest==1,] = xtest[ytest==1,] + 1
xtest
testdat <- data.frame(x = xtest, y = as.factor(ytest))
testdat
ypred <- predict(bestmod,testdat)
help(predict)
svmfit$index
attributes(svmfit)
svm$fitted
svmfit$fitted
svmfit$labels
svmfit$epsilon
svmfit$cost
svmfit$decision.values
set.seed(1)
tune.out <- tune(svm,y~., data=dat, kernel="linear", ranges=list(cost=c(0.001,0.01,0.1,1,5,10,100)))
summary(tune.out)
attributes(tune.out)
tune.out$best.performance
tune.out$performances
tune.out$best.model
bestmod <- tune.out$best.model
ypred <- predict(bestmod,testdat)
ypred
CrossTable(x = ytest, y = ypred, prop.chisq=FALSE)
ytest==1
table(predict=ypred,truth=testdat$y)
testdat$y
CrossTable(x = testdat$y, y = ypred, prop.chisq=FALSE)
set.seed(1)
x <- matrix(rnorm(20*2),ncol=2)    # create data matrix
y <- c(rep(-1,10),rep(1,10))       # create label vector
x[y==1,] = x[y==1,] + 1
# Are the classes linearlly separable?
plot(x,col=3-y)                    # From this plot, the answer is no
# Next we fit the support vector classifier. Note for svm() to do classification, we need to encode the response as a factor variable
dat <- data.frame(x=x, y= as.factor(y))
library(e1071)
svmfit <- svm(y~., data=dat, kernel="linear", cost=10, scale=FALSE)
plot(svmfit,dat)
svmfit$index
set.seed(1)
tune.out <- tune(svm,y~., data=dat, kernel="linear", ranges=list(cost=c(0.001,0.01,0.1,1,5,10,100)))
summary(tune.out)
bestmod <- tune.out$best.model
xtest <- matrix(rnorm(20*2),ncol=2)
ytest <- sample(c(-1,1),20,rep=TRUE)
xtest[ytest==1,] = xtest[ytest==1,] + 1
testdat <- data.frame(x = xtest, y = as.factor(ytest))
ypred <- predict(bestmod,testdat)
table(predict=ypred,truth=testdat$y)
CrossTable(x = ytest, y = ypred, prop.chisq=FALSE)
svmfit<- svm(y~., data=dat, kernel="linear",cost=0.01,scale=FALSE)
ypred <- predict(svmfit,testdat)
CrossTable(x = testdat$y, y = ypred, prop.chisq=FALSE)
letter
letters <- read.csv("chapter 7/letterdata.csv")
letters_train <- letters[1:16000, ]
letters_test <- letters[16001:20000, ]
attributes(letters_test)
letters_test$letter
str(letters_test)
letter_classifier <- ksvm(letter ~ ., data = letters_train, kernel = "vanilladot")
# STEP 4: Evaluate model performance
letter_predictions <- predict(letter_classifier, letters_test)
# This returns a vector containing a predicted letter for each row of values in the testing data
# Now, let's compare the predicted letter to the true letter in the testing dataset:
table(letter_predictions, letters_test$letter)
