help()
mean(abs(rnorm(100)))
mean(abs(rnorm(100)))
rnorm(100)
abs(rnorm(10)
quit
abs(rnorm(10))
plot(abs(rnorm(10)))
plot(rnorm(10))
x <- c(1,2,4)
q <- c(x,x,7)
q
x
x[3]
q[5]
x
x[2:3]
mean(x)
sd(x)
avg(x)
y<- mean(x)
y
clear(q)
y # print out y
setwd("~/R Files")
data()
mean(Nile)
hist(Nile)
hist(women)
plot(women)
hist(women,breaks=12)
plot(women,breaks=12)
q()
read.csv(kerrSdgePowerUseage.csv)
read.csv("KerrSdgePowerUseage.csv"")
read.csv("KerrSdgePowerUseage.csv")
str(KerrSdgePowerUseage.csv)
str("KerrSdgePowerUseage.csv"")
str("KerrSdgePowerUseage.csv")
pwr_tbl <- read.csv("KerrSdgePowerUseage.csv")
str(pwr_tbl)
pwr_tbl
kwh <- pwr_tbl[,4]
kwh
summary(kwh)
d+d
2+2
install.packages("robust")
data(breslow.dat, package="robust")
names(breslow.dat)
str(breslow.dat)
library(ggplot2)
load(diamonds)
data(diamonds)
plot(diamonds$carat)
hist(diamonds$carat, xlab = 'Carat')
plot(diamonds$carat, diamonds$price)
ggplot(diamonds=diamonds) + geom_histogram(aes(x=carat))
ggplot(data=diamonds) + geom_histogram(aes(x=carat))
ggplot(data=diamonds) + geom_density(aes(x=carat))
ggplot(data=diamonds) + geom_density(aes(x=carat), fill='grey50')
ggplot(data=diamonds) + geom_density(aes(x=carat), fill='grey40')
ggplot(data=diamonds) + geom_density(aes(x=carat), fill='grey20')
ggplot(data=diamonds) + geom_density(aes(x=carat), fill='grey60')
ggplot(diamonds, aes(x=carat,y=price)) + geom_point()
g <- ggplot(diamonds,aes(x=carat,y=price))
g + geom_point(aes(color=color))
g + geom_point()
g + geom_point(aes(color=color))
g + geom_point(aes(color=carat))
str(diamonds)
g + geom_point(aes(color=clarity))
say.hello <- function() {
print("Hello, World!")
}
say.hello
say.hello()
sprintf('Hello' %s, 'World')
sprintf('Hello %s', 'World')
hello.person <- function(name) {
sprintf('Hello %s', 'name')
}
hello.person(Macy)
hello.person <- function(name) {
sprintf('Hello %s', name)
}
hello.person(Macy)
hello.person("Macy")
double.num <- function(x)
{
x * 2
}
double.num(4)
double.num <- function(x)
{
return(x * 2)
}
double.num(4)
double.list <- function(list)
{
return(list * 2)
}
x <- c(1,2,3,4)
double.list(x)
```{r}
summary(cars)
plot(cars)
df <- data.frame(x = 1:3, y = 3:1, z = letters[1:3])
df
df[df$x == 2,]
melt(df)
library(reshape2)
melt(df)
df[c(1,3),]
df
df[df$x == 3,]
mtcars
str(mtcars)
mtcars[mtcars$cyl == 4,]
mtcars[mtcars$hp > 100,]
mtcars[mtcars$hp > 200,]
head(mtcars)
names(mtcars)
mtcars[is.na(mtcars$mpg)]
is.na(mtcars$mgs)
apply(mtcars,2,is.na)
apply(mtcars,2,is.na)
x <- c(1,1,2,3,5,8)
x[c(T,T,F,F,T,T)]
a <- matrix(1:9, nrow = 3)
a
colnames(a) <- c("A","B", "C")
a
a[c(T,F,T), c("B","A")]
df
str(df)
df[df$y > 1,]
mtcars
head(mtcars)
mtcars[mtcars$cyl == 8]
mtcars[,mtcars$cyl == 8]
mtcars[,mtcars$cyl == 8,]
mtcars[mtcars$cyl == 8,]
mtcars[, mtcars$cyl == 8]
mtcars[mtcars$cyl == 8,]
melt(mtcars)
help(pam)
help(cluster)
library(cluster)
help(cluster)
df1 = data.frame(CustomerId=c(1:6),Product=c(rep("Toaster",3),rep("Radio",3)))
df2 = data.frame(CustomerId=c(2,4,6),State=c(rep("Alabama",2),rep("Ohio",1)))
df1
df2
outer <- merge(x = df1, y = df2, by = "CustomerId", all = TRUE)
outer
outer[is.na(outer$State)]
outer$State
outer[is.na(,outer$State)]
outer[is.na(outer$State)]
is.na(outer$State)
outer$State(is.na(outer$State))
outer$State[is.na(outer$State)]
outer$State[is.na(outer$State)] <- 0
outer$State[is.na(outer$State) <- 0]
outer
left.outer <- merge(x = df1, y = df2, by = "CustomerId", all.x=TRUE)
left.outer
right.outer <- merge(x = df1, y = df2, by = "CustomerId", all.y=TRUE)
right.outer
cross.join <- merge(x = df1, y = df2, by = NULL)
cross.join
setwd("/Users/stuart/Desktop/BigDataDocumentation/DataSets//MachineLearningWithRDatasets/")
library(gmodels)   # For CrossTables
library(class)     # For kNN classification
usedcars <- read.csv("chapter 2/usedcars.csv", stringsAsFactors = FALSE)
usedcars$conservative <- usedcars$color %in% c("Black","Gray","Silver","White")
table(usedcars$conservative)
CrossTable(x = usedcars$model, y=usedcars$conservative)
wbcd <- read.csv("chapter 3/wisc_bc_data.csv", stringsAsFactors = FALSE)
str(wbcd)
head(wbcd, n = 10L)
wbcd <- wbcd[-1]
table(wbcd$diagnosis)
# Many R ML classifiers require that the "target feature" is coded as a factor
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B","M"), labels = c("Benign", "Malignant"))
prop.table(table(wbcd$diagnosis))*100
# Let's take a look at some of the features (since numeric, we use summary)
summary(wbcd[c("radius_mean","area_mean", "smoothness_mean")])
normalize <- function(x){
return( (x - min(x))/(max(x) - min(x)) )
}
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
summary(wbcd_n[c("radius_mean","area_mean", "smoothness_mean")])
wbcd_train <- wbcd_n[1:469,]
wbcd_test <- wbcd_n[470:569,]
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
head(wbcd_test)
head(wbcd_train)
str(wbcd_n)
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
# Step 3: Training a model on the data
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
letters <- read.csv("chapter 7/letterdata.csv")
str(letters)
letters_train <- letters[1:16000, ]
letters_test <- letters[16001:20000, ]
install.packages("kernlab")
library(kernlab)
letter_classifier <- ksvm(letter ~ ., data = letters_train, kernel = "vanilladot")
letter_classifier
letter_predictions <- predict(letter_classifier, letters_test)
head(letter_predictions)
table(letter_predictions, letters_test$letter)
